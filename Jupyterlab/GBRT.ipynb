{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec80a8b-6b7e-45b5-b39c-9297d849b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import gauss\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "sys.path.insert(2,'..')\n",
    "import functions\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "import annualized_rv as arv\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0951eacd-7082-4a67-a683-74134f8b8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaplHistIV = pd.read_pickle('historicImpliedVolData/aapl_mean_iv_2017_2022.pkl')\n",
    "googHistIV = pd.read_pickle('historicImpliedVolData/goog_mean_iv_2017_2022.pkl')\n",
    "msftHistIV = pd.read_pickle('historicImpliedVolData/msft_mean_iv_2017_2022.pkl')\n",
    "ndxHistIV = pd.read_pickle('historicImpliedVolData/ndx_mean_iv_2017_2022.pkl')\n",
    "spyHistIV = pd.read_pickle('historicImpliedVolData/spc_mean_iv_2017_2022.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6732971-0c58-45e2-a845-dd512510aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spyHistIV.rename(columns = {'date':'Date'},inplace = True) #renaming date column to Date for consistency\n",
    "spyHistIV.set_index('Date',inplace = True,drop = True) #setting index to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a40d77-8c2c-4225-acae-6729e1a0c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#grab spy  from yfinance\n",
    "\n",
    "spyHistory = yf.download('^GSPC', start='2016-01-01', end='2023-12-31')\n",
    "#calculate realised vol\n",
    "window =21 #realisedVol window size\n",
    "spyHistory['Daily Return'] = spyHistory['Adj Close'].pct_change()\n",
    "spyHistory['21dRealisedVol'] = spyHistory['Daily Return'].rolling(window=window).std() * np.sqrt(252)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e6e227-b241-46ac-a218-839545b3a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "historicIVSeries = spyHistIV['average_iv']\n",
    "historicVolumeSeries = spyHistory['Volume'].rolling(21).mean()['2017':'2021']\n",
    "dailyReturnSeries= spyHistory['Daily Return']['2017':'2021']\n",
    "dailyRealisedVolSeries = spyHistory['21dRealisedVol']['2017':'2021']\n",
    "df_combined = pd.concat([historicIVSeries,dailyReturnSeries,dailyRealisedVolSeries,historicVolumeSeries], axis=1)\n",
    "\n",
    "# Scale the data\n",
    "scaler = preprocessing.StandardScaler().fit(df_combined)\n",
    "scaled_data = scaler.transform(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cb8233-b8ef-4e50-93bf-2dcfde9618bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scaled_data\n",
    "seq_length = 60\n",
    "n_features = data.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bc8ed6-e1e3-4bbc-a6cb-acd016ac03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length - 3):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[(i + seq_length), 0]  # Next 3-day IV\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f0ce50-72d7-4d9e-8c47-58f1e56c5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_sequences(data, seq_length)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5e66c1-2de1-45e4-9b80-bef9b608b599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f55a2891-18cd-43bc-94fe-1cb4acb604c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8989dc5c-42ff-40a7-82bf-029f8709e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFT(pl.LightningModule):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(TFT, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(model_dim, 1)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = src * torch.sqrt(torch.tensor(self.model_dim, dtype=torch.float32))\n",
    "        src = self.transformer_encoder(src)\n",
    "        output = self.linear(src)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "159453c5-32e0-440c-8dd1-6a601ab7d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6981201e-999c-481e-b6da-6507c093ae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_features = X_train.shape[2]\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) \n",
    "\n",
    "\n",
    "model = TFT(input_dim=n_features, model_dim=4, num_heads=4, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e677c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Ahmed\\miniconda3\\envs\\myenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                    | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | encoder_layer       | TransformerEncoderLayer | 18.5 K | train\n",
      "1 | transformer_encoder | TransformerEncoder      | 37.1 K | train\n",
      "2 | linear              | Linear                  | 5      | train\n",
      "------------------------------------------------------------------------\n",
      "55.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.6 K    Total params\n",
      "0.222     Total estimated model params size (MB)\n",
      "C:\\Users\\Ahmed\\miniconda3\\envs\\myenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\Ahmed\\miniconda3\\envs\\myenv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a85ea6cf37f45409e7e66a978c41927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15771a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7a588",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Data type of X_train:\", X_train.dtype)\n",
    "print(\"Data type of y_train:\", y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839721d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
